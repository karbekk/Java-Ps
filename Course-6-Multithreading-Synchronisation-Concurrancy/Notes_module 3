Read Write operations ordering:

Synchronisation protects block of code. It guarantees this block of code is executed by one thread at a time.
So it prevents 2 threads from executing at the same time thus preventing race condition.

Now a days read and write operations are critical and are diff from age old CPU's. Because CPU's does not read a variable
from main memory, but from internal cache.

CPU Architecture:

We have main memory and CPU which has sub CPU's on it called cores.

they are linked using bus.

Inside CPU, we can have many cores. Lets say we have 4 cores, core 1 , core 2, core 3, core 4

each core has several layers of memory caches called L1,L2s.

There is a common layer cache for all the cores called L3.

This is the typical design. The caches allow much faster access than accessing from main memory.
Eg: Access to main memory is -> 100ns
    Access to L2 cache       -> 7ns
    Access to L1 cache       -> 0.5ns

Trade-off: Amount of memory on main memory is    : several GB
                                    L2 Cache is  : 256 kbs
                                    L1 Cache     : 32kb


Eg:

we have count variable initialised to 0 in main memory.
The Producer is running on Core 1. It needs to access the the count variable which will be copied to L1 cache.
Core 1 increments the value of count on cache l1 from 0 to 1.

Core 2 which has consumer also needs same count variable. Now count variable is placed in 2 places in my CPU.
Core 2 should get the incremented value of 1 not the value 0 from main memory. Because core 1 has not yet written
back the value 1 to main memory as its slow.

This is called as visibility. Visibility is about informing the other caches of my CPU that variable has been modified and
right value is in one of the caches of the CPU and should not be fetched from the main memory.

Visibility: a read should return the value set by last write.

so for a read and write operations to preserve visibility:

1. make the methods synchronised
2. make the shared variable volatile.

the "happens before" link is very imp. if there is no link values can be diff, otherwise threads follow the order.

All the shared variables must be synchronised or volatile for access.

False Sharing:
----------------
1. Happens because of the way CPU caches work
2. Its a side effect that can have effect on performance

Caches are organised as line of data inside the CPU
Each line can hold 8 longs. (64 bytes)

When a visible variable is modified in L1 cache, all the line is marked "dirty" for other caches, thus, a read
on dirty line causes refresh on L1 line.

Eg: Core 1 has method 1 running on it. L1 thread is executing the method. it loads a line of cache from main memory.
    Core 2 has method 2 running on it. L2 thread is executing the method. it loads a line of cache from main memory.

    But the way memory is arranged, cache in core 1  gets loaded with a,b contiguously.
                                    cache in core 2 gets loaded with b,a contiguously.

    Now, a++ on Core 1 cache, causes dirty lines on core 2 cache.
    so core 2 cannot run b++ on it. because it has dirty line on it caused by core 1 cache even though core 1 cache
    never touched variable b.
    therefore the core 2 now has to read from main memory instead of reading from its cache. This is known as Cache Miss.
    This mechanism is known as False Sharing.

    This is hard to predict and hits performance. it happens in invisible way.

    ---------------------------------------------------------------------------------------------











